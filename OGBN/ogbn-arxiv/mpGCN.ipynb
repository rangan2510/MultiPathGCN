{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:42:44.827349Z",
     "iopub.status.busy": "2020-09-16T21:42:44.826508Z",
     "iopub.status.idle": "2020-09-16T21:43:04.501128Z",
     "shell.execute_reply": "2020-09-16T21:43:04.499254Z"
    },
    "papermill": {
     "duration": 19.687702,
     "end_time": "2020-09-16T21:43:04.501281",
     "exception": false,
     "start_time": "2020-09-16T21:42:44.813579",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dgl-cu101\r\n",
      "  Downloading dgl_cu101-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.5 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 25.5 MB 6.6 MB/s \r\n",
      "\u001b[?25hCollecting ogb\r\n",
      "  Downloading ogb-1.2.3-py3-none-any.whl (55 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 55 kB 2.5 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (1.18.5)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (2.23.0)\r\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (2.4)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (1.4.1)\r\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.45.0)\r\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.24.3)\r\n",
      "Collecting outdated>=0.2.0\r\n",
      "  Downloading outdated-0.2.0.tar.gz (4.0 kB)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.14.0)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.5.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.23.2)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.1.1)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl-cu101) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl-cu101) (2020.6.20)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl-cu101) (2.9)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.1->dgl-cu101) (4.4.2)\r\n",
      "Collecting littleutils\r\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->ogb) (0.18.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\r\n",
      "Building wheels for collected packages: outdated, littleutils\r\n",
      "  Building wheel for outdated (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for outdated: filename=outdated-0.2.0-py3-none-any.whl size=4960 sha256=7590563b36397d48a16c276c4368762856764d69728f030d5619ce98e097536c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6f/cd/a2/e49170b2cf59e88b952f3414f25a54d9f16f033bded4aaab26\r\n",
      "  Building wheel for littleutils (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=137a9dcc3800af5f394687213d807a56a858d96f8ed44b09eaf3e12ebc07021b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\r\n",
      "Successfully built outdated littleutils\r\n",
      "Installing collected packages: dgl-cu101, littleutils, outdated, ogb\r\n",
      "Successfully installed dgl-cu101-0.5.2 littleutils-0.2.2 ogb-1.2.3 outdated-0.2.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "! pip install dgl-cu101 ogb\n",
    "ROOT = \"/kaggle/input/dgl-ogbnarxiv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:43:04.628413Z",
     "iopub.status.busy": "2020-09-16T21:43:04.627563Z",
     "iopub.status.idle": "2020-09-16T21:43:07.992348Z",
     "shell.execute_reply": "2020-09-16T21:43:07.991154Z"
    },
    "papermill": {
     "duration": 3.425917,
     "end_time": "2020-09-16T21:43:07.992478",
     "exception": false,
     "start_time": "2020-09-16T21:43:04.566561",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "from dgl.data.utils import load_graphs\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "from dgl.nn import GraphConv, SAGEConv\n",
    "from ogb.nodeproppred import Evaluator\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "x,_ = load_graphs(ROOT + \"data.bin\")\n",
    "graph = x[0]\n",
    "\n",
    "train_idx = torch.load(ROOT + 'train_idx.pt')\n",
    "test_idx = torch.load(ROOT + 'test_idx.pt')\n",
    "val_idx = torch.load(ROOT + 'val_idx.pt')\n",
    "labels = torch.load(ROOT + 'labels.pt')\n",
    "\n",
    "splitted_idx = {'train':train_idx, 'test':test_idx, 'valid':val_idx}\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:43:08.102410Z",
     "iopub.status.busy": "2020-09-16T21:43:08.101803Z",
     "iopub.status.idle": "2020-09-16T21:43:08.124681Z",
     "shell.execute_reply": "2020-09-16T21:43:08.123833Z"
    },
    "papermill": {
     "duration": 0.080498,
     "end_time": "2020-09-16T21:43:08.124817",
     "exception": false,
     "start_time": "2020-09-16T21:43:08.044319",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 40\n"
     ]
    }
   ],
   "source": [
    "in_feats = graph.ndata[\"feat\"].shape[1]\n",
    "n_classes = (labels.max() + 1).item()\n",
    "print (in_feats, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:43:08.251960Z",
     "iopub.status.busy": "2020-09-16T21:43:08.250460Z",
     "iopub.status.idle": "2020-09-16T21:43:08.253125Z",
     "shell.execute_reply": "2020-09-16T21:43:08.253610Z"
    },
    "papermill": {
     "duration": 0.076953,
     "end_time": "2020-09-16T21:43:08.253744",
     "exception": false,
     "start_time": "2020-09-16T21:43:08.176791",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    print ('random seed set to be ' + str(seed))\n",
    "    \n",
    "def cross_entropy(x, labels):\n",
    "    x = x.to(device)\n",
    "    labels = labels.to(device)\n",
    "    y = F.cross_entropy(x, labels[:, 0], reduction=\"none\")\n",
    "    y = th.log(0.5 + y) - math.log(0.5)\n",
    "    return th.mean(y)\n",
    "\n",
    "def compute_acc(pred, labels, evaluator):\n",
    "    return evaluator.eval({\"y_pred\": pred.argmax(dim=-1, keepdim=True), \"y_true\": labels})[\"acc\"]\n",
    "\n",
    "def add_labels(feat, labels, idx):\n",
    "    onehot = th.zeros([feat.shape[0], n_classes]).to(device)\n",
    "    onehot[idx, labels[idx, 0]] = 1\n",
    "    return th.cat([feat, onehot], dim=-1)\n",
    "\n",
    "def train(model, graph, labels, train_idx, optimizer):\n",
    "    model.train()\n",
    "    feat = graph.ndata[\"feat\"]\n",
    "    mask_rate = 0.5\n",
    "    mask = th.rand(train_idx.shape) < mask_rate\n",
    "    train_labels_idx = train_idx[mask]\n",
    "    train_pred_idx = train_idx[~mask]\n",
    "    feat = add_labels(feat, labels, train_labels_idx)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(graph, feat)\n",
    "    loss = cross_entropy(pred[train_pred_idx], labels[train_pred_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, pred\n",
    "\n",
    "@th.no_grad()\n",
    "def evaluate(model, graph, labels, train_idx, val_idx, test_idx, evaluator):\n",
    "    model.eval()\n",
    "    feat = graph.ndata[\"feat\"]\n",
    "    feat = add_labels(feat, labels, train_idx)\n",
    "    pred = model(graph, feat)\n",
    "    train_loss = cross_entropy(pred[train_idx], labels[train_idx])\n",
    "    val_loss = cross_entropy(pred[val_idx], labels[val_idx])\n",
    "    test_loss = cross_entropy(pred[test_idx], labels[test_idx])\n",
    "\n",
    "    return (\n",
    "        compute_acc(pred[train_idx], labels[train_idx], evaluator),\n",
    "        compute_acc(pred[val_idx], labels[val_idx], evaluator),\n",
    "        compute_acc(pred[test_idx], labels[test_idx], evaluator),\n",
    "        train_loss,\n",
    "        val_loss,\n",
    "        test_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:43:08.373172Z",
     "iopub.status.busy": "2020-09-16T21:43:08.370963Z",
     "iopub.status.idle": "2020-09-16T21:43:08.738477Z",
     "shell.execute_reply": "2020-09-16T21:43:08.739107Z"
    },
    "papermill": {
     "duration": 0.432585,
     "end_time": "2020-09-16T21:43:08.739243",
     "exception": false,
     "start_time": "2020-09-16T21:43:08.306658",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed set to be 0\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(0)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,in_feats,out_feats,num_channels=8):\n",
    "        super(Block, self).__init__()\n",
    "        self.gc = GraphConv(in_feats, out_feats)\n",
    "        \n",
    "    def forward(self, g, node_state):\n",
    "        node_states = self.gc(g, node_state)\n",
    "        node_states = F.relu(node_states)\n",
    "        node_states = F.dropout(node_states, p=0.5, training=self.training)\n",
    "        return node_states\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, out_feats,num_paths=2):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_paths = num_paths\n",
    "        self.init = Block(in_feats, n_hidden)\n",
    "        self.lyrs_0 = nn.ModuleList()\n",
    "        self.lyrs_1 = nn.ModuleList()\n",
    "        for _ in range(3):\n",
    "            self.lyrs_0.append(Block(n_hidden, n_hidden))\n",
    "        self.lyrs_1.append(Block(n_hidden, n_hidden//2))\n",
    "        self.lyrs_1.append(Block(n_hidden//2, n_hidden))\n",
    "        self.lin = nn.Linear(n_hidden, out_feats)\n",
    "        \n",
    "    def forward(self, g, feat):\n",
    "        node_state = feat\n",
    "        out = _out = __out = self.init(g, node_state)\n",
    "        for i in range(3):\n",
    "            out = self.lyrs_0[i](g, out)\n",
    "        for lyr in self.lyrs_1:\n",
    "            _out = lyr(g, _out)\n",
    "            \n",
    "        out = torch.stack([out, _out],dim=1).sum(1)\n",
    "        \n",
    "        out = self.lin(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:43:08.856220Z",
     "iopub.status.busy": "2020-09-16T21:43:08.855370Z",
     "iopub.status.idle": "2020-09-16T21:43:14.460085Z",
     "shell.execute_reply": "2020-09-16T21:43:14.461139Z"
    },
    "papermill": {
     "duration": 5.665647,
     "end_time": "2020-09-16T21:43:14.461317",
     "exception": false,
     "start_time": "2020-09-16T21:43:08.795670",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduler min_lr 0.001\n"
     ]
    }
   ],
   "source": [
    "warmup_epochs = 10\n",
    "num_epochs = 1000\n",
    "patience = 250\n",
    "log_every = 10\n",
    "lr = 1e-2\n",
    "weight_decay = 0\n",
    "\n",
    "model = Net(in_feats=128 + n_classes, n_hidden=256, out_feats=40).to(device)\n",
    "graph = graph.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "min_lr = 1e-3\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.7, patience=150,  verbose=True, cooldown=0, min_lr=min_lr)\n",
    "print ('scheduler min_lr', min_lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "evaluator = Evaluator('ogbn-arxiv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:43:14.636742Z",
     "iopub.status.busy": "2020-09-16T21:43:14.635357Z",
     "iopub.status.idle": "2020-09-16T21:49:19.829788Z",
     "shell.execute_reply": "2020-09-16T21:49:19.830392Z"
    },
    "papermill": {
     "duration": 365.283662,
     "end_time": "2020-09-16T21:49:19.830572",
     "exception": false,
     "start_time": "2020-09-16T21:43:14.546910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 epochs...\n",
      "Done in 3.27 sec\n",
      "Epoch 10\t loss 1.4781\t train_acc 0.3762\t val_acc 0.3626\t test_acc 0.3260\t dur 0.36s\t patience 0\n",
      "Epoch 20\t loss 1.2226\t train_acc 0.5789\t val_acc 0.5623\t test_acc 0.5524\t dur 0.36s\t patience 0\n",
      "Epoch 30\t loss 1.0591\t train_acc 0.6320\t val_acc 0.6012\t test_acc 0.5917\t dur 0.36s\t patience 0\n",
      "Epoch 40\t loss 0.9480\t train_acc 0.6879\t val_acc 0.6369\t test_acc 0.6230\t dur 0.36s\t patience 1\n",
      "Epoch 50\t loss 0.8669\t train_acc 0.7325\t val_acc 0.6689\t test_acc 0.6555\t dur 0.36s\t patience 1\n",
      "Epoch 60\t loss 0.8011\t train_acc 0.7560\t val_acc 0.6774\t test_acc 0.6564\t dur 0.36s\t patience 2\n",
      "Epoch 70\t loss 0.7774\t train_acc 0.7725\t val_acc 0.6945\t test_acc 0.6735\t dur 0.36s\t patience 5\n",
      "Epoch 80\t loss 0.7510\t train_acc 0.7834\t val_acc 0.7028\t test_acc 0.6856\t dur 0.36s\t patience 1\n",
      "Epoch 90\t loss 0.7282\t train_acc 0.7903\t val_acc 0.7106\t test_acc 0.6931\t dur 0.36s\t patience 0\n",
      "Epoch 100\t loss 0.7192\t train_acc 0.7955\t val_acc 0.7149\t test_acc 0.6955\t dur 0.36s\t patience 1\n",
      "Epoch 110\t loss 0.7085\t train_acc 0.8000\t val_acc 0.7172\t test_acc 0.6960\t dur 0.36s\t patience 11\n",
      "Epoch 120\t loss 0.7003\t train_acc 0.8011\t val_acc 0.7152\t test_acc 0.6923\t dur 0.36s\t patience 2\n",
      "Epoch 130\t loss 0.6908\t train_acc 0.8039\t val_acc 0.7206\t test_acc 0.7002\t dur 0.36s\t patience 6\n",
      "Epoch 140\t loss 0.6853\t train_acc 0.8060\t val_acc 0.7226\t test_acc 0.7042\t dur 0.36s\t patience 5\n",
      "Epoch   152: reducing learning rate of group 0 to 7.0000e-03.\n",
      "Epoch 150\t loss 0.6838\t train_acc 0.8073\t val_acc 0.7242\t test_acc 0.7052\t dur 0.37s\t patience 15\n",
      "Epoch 160\t loss 0.6790\t train_acc 0.8079\t val_acc 0.7256\t test_acc 0.7075\t dur 0.36s\t patience 0\n",
      "Epoch 170\t loss 0.6790\t train_acc 0.8085\t val_acc 0.7255\t test_acc 0.7057\t dur 0.36s\t patience 3\n",
      "Epoch 180\t loss 0.6680\t train_acc 0.8089\t val_acc 0.7251\t test_acc 0.7086\t dur 0.36s\t patience 13\n",
      "Epoch 190\t loss 0.6634\t train_acc 0.8094\t val_acc 0.7229\t test_acc 0.7039\t dur 0.36s\t patience 23\n",
      "Epoch 200\t loss 0.6671\t train_acc 0.8103\t val_acc 0.7253\t test_acc 0.7080\t dur 0.36s\t patience 33\n",
      "Epoch 210\t loss 0.6647\t train_acc 0.8100\t val_acc 0.7272\t test_acc 0.7113\t dur 0.36s\t patience 3\n",
      "Epoch 220\t loss 0.6601\t train_acc 0.8107\t val_acc 0.7235\t test_acc 0.7011\t dur 0.36s\t patience 6\n",
      "Epoch 230\t loss 0.6547\t train_acc 0.8111\t val_acc 0.7267\t test_acc 0.7076\t dur 0.36s\t patience 16\n",
      "Epoch 240\t loss 0.6546\t train_acc 0.8124\t val_acc 0.7291\t test_acc 0.7133\t dur 0.36s\t patience 26\n",
      "Epoch 250\t loss 0.6579\t train_acc 0.8098\t val_acc 0.7250\t test_acc 0.7043\t dur 0.36s\t patience 36\n",
      "Epoch 260\t loss 0.6518\t train_acc 0.8112\t val_acc 0.7274\t test_acc 0.7067\t dur 0.36s\t patience 46\n",
      "Epoch 270\t loss 0.6485\t train_acc 0.8115\t val_acc 0.7271\t test_acc 0.7067\t dur 0.36s\t patience 56\n",
      "Epoch 280\t loss 0.6470\t train_acc 0.8125\t val_acc 0.7296\t test_acc 0.7094\t dur 0.36s\t patience 66\n",
      "Epoch 290\t loss 0.6489\t train_acc 0.8124\t val_acc 0.7295\t test_acc 0.7122\t dur 0.36s\t patience 1\n",
      "Epoch   303: reducing learning rate of group 0 to 4.9000e-03.\n",
      "Epoch 300\t loss 0.6538\t train_acc 0.8106\t val_acc 0.7215\t test_acc 0.6944\t dur 0.36s\t patience 2\n",
      "Epoch 310\t loss 0.6438\t train_acc 0.8115\t val_acc 0.7277\t test_acc 0.7051\t dur 0.36s\t patience 12\n",
      "Epoch 320\t loss 0.6360\t train_acc 0.8109\t val_acc 0.7290\t test_acc 0.7078\t dur 0.38s\t patience 22\n",
      "Epoch 330\t loss 0.6460\t train_acc 0.8116\t val_acc 0.7260\t test_acc 0.7035\t dur 0.36s\t patience 32\n",
      "Epoch 340\t loss 0.6360\t train_acc 0.8120\t val_acc 0.7271\t test_acc 0.7057\t dur 0.36s\t patience 42\n",
      "Epoch 350\t loss 0.6418\t train_acc 0.8124\t val_acc 0.7298\t test_acc 0.7082\t dur 0.38s\t patience 4\n",
      "Epoch 360\t loss 0.6380\t train_acc 0.8127\t val_acc 0.7321\t test_acc 0.7146\t dur 0.36s\t patience 9\n",
      "Epoch 370\t loss 0.6393\t train_acc 0.8122\t val_acc 0.7321\t test_acc 0.7136\t dur 0.36s\t patience 19\n",
      "Epoch 380\t loss 0.6297\t train_acc 0.8124\t val_acc 0.7299\t test_acc 0.7090\t dur 0.39s\t patience 29\n",
      "Epoch 390\t loss 0.6340\t train_acc 0.8127\t val_acc 0.7282\t test_acc 0.7064\t dur 0.36s\t patience 39\n",
      "Epoch 400\t loss 0.6367\t train_acc 0.8132\t val_acc 0.7311\t test_acc 0.7088\t dur 0.36s\t patience 49\n",
      "Epoch 410\t loss 0.6285\t train_acc 0.8126\t val_acc 0.7321\t test_acc 0.7102\t dur 0.37s\t patience 59\n",
      "Epoch 420\t loss 0.6323\t train_acc 0.8128\t val_acc 0.7281\t test_acc 0.7065\t dur 0.36s\t patience 69\n",
      "Epoch 430\t loss 0.6352\t train_acc 0.8129\t val_acc 0.7301\t test_acc 0.7085\t dur 0.36s\t patience 79\n",
      "Epoch 440\t loss 0.6264\t train_acc 0.8126\t val_acc 0.7296\t test_acc 0.7086\t dur 0.36s\t patience 89\n",
      "Epoch   454: reducing learning rate of group 0 to 3.4300e-03.\n",
      "Epoch 450\t loss 0.6277\t train_acc 0.8132\t val_acc 0.7323\t test_acc 0.7144\t dur 0.36s\t patience 99\n",
      "Epoch 460\t loss 0.6319\t train_acc 0.8139\t val_acc 0.7320\t test_acc 0.7114\t dur 0.36s\t patience 109\n",
      "Epoch 470\t loss 0.6236\t train_acc 0.8143\t val_acc 0.7347\t test_acc 0.7159\t dur 0.36s\t patience 119\n",
      "Epoch 480\t loss 0.6271\t train_acc 0.8135\t val_acc 0.7328\t test_acc 0.7133\t dur 0.36s\t patience 129\n",
      "Epoch 490\t loss 0.6357\t train_acc 0.8144\t val_acc 0.7343\t test_acc 0.7151\t dur 0.36s\t patience 139\n",
      "Epoch 500\t loss 0.6285\t train_acc 0.8140\t val_acc 0.7316\t test_acc 0.7098\t dur 0.36s\t patience 149\n",
      "Epoch 510\t loss 0.6285\t train_acc 0.8144\t val_acc 0.7327\t test_acc 0.7125\t dur 0.36s\t patience 159\n",
      "Epoch 520\t loss 0.6309\t train_acc 0.8151\t val_acc 0.7350\t test_acc 0.7159\t dur 0.36s\t patience 169\n",
      "Epoch 530\t loss 0.6252\t train_acc 0.8143\t val_acc 0.7337\t test_acc 0.7141\t dur 0.36s\t patience 179\n",
      "Epoch 540\t loss 0.6221\t train_acc 0.8144\t val_acc 0.7340\t test_acc 0.7132\t dur 0.36s\t patience 189\n",
      "Epoch 550\t loss 0.6220\t train_acc 0.8138\t val_acc 0.7353\t test_acc 0.7177\t dur 0.36s\t patience 5\n",
      "Epoch 560\t loss 0.6255\t train_acc 0.8141\t val_acc 0.7354\t test_acc 0.7191\t dur 0.36s\t patience 0\n",
      "Epoch 570\t loss 0.6281\t train_acc 0.8144\t val_acc 0.7351\t test_acc 0.7172\t dur 0.36s\t patience 10\n",
      "Epoch 580\t loss 0.6250\t train_acc 0.8141\t val_acc 0.7337\t test_acc 0.7151\t dur 0.37s\t patience 20\n",
      "Epoch 590\t loss 0.6272\t train_acc 0.8143\t val_acc 0.7349\t test_acc 0.7140\t dur 0.36s\t patience 30\n",
      "Epoch   605: reducing learning rate of group 0 to 2.4010e-03.\n",
      "Epoch 600\t loss 0.6171\t train_acc 0.8143\t val_acc 0.7343\t test_acc 0.7143\t dur 0.36s\t patience 40\n",
      "Epoch 610\t loss 0.6185\t train_acc 0.8135\t val_acc 0.7316\t test_acc 0.7099\t dur 0.36s\t patience 50\n",
      "Epoch 620\t loss 0.6208\t train_acc 0.8143\t val_acc 0.7328\t test_acc 0.7103\t dur 0.36s\t patience 2\n",
      "Epoch 630\t loss 0.6155\t train_acc 0.8144\t val_acc 0.7350\t test_acc 0.7138\t dur 0.36s\t patience 12\n",
      "Epoch 640\t loss 0.6233\t train_acc 0.8143\t val_acc 0.7351\t test_acc 0.7148\t dur 0.36s\t patience 5\n",
      "Epoch 650\t loss 0.6191\t train_acc 0.8135\t val_acc 0.7329\t test_acc 0.7095\t dur 0.36s\t patience 15\n",
      "Epoch 660\t loss 0.6234\t train_acc 0.8138\t val_acc 0.7335\t test_acc 0.7120\t dur 0.36s\t patience 2\n",
      "Epoch 670\t loss 0.6174\t train_acc 0.8143\t val_acc 0.7355\t test_acc 0.7155\t dur 0.36s\t patience 12\n",
      "Epoch 680\t loss 0.6204\t train_acc 0.8146\t val_acc 0.7349\t test_acc 0.7161\t dur 0.36s\t patience 22\n",
      "Epoch 690\t loss 0.6199\t train_acc 0.8148\t val_acc 0.7342\t test_acc 0.7131\t dur 0.36s\t patience 32\n",
      "Epoch 700\t loss 0.6185\t train_acc 0.8144\t val_acc 0.7317\t test_acc 0.7081\t dur 0.36s\t patience 42\n",
      "Epoch 710\t loss 0.6143\t train_acc 0.8140\t val_acc 0.7344\t test_acc 0.7120\t dur 0.36s\t patience 7\n",
      "Epoch 720\t loss 0.6154\t train_acc 0.8141\t val_acc 0.7335\t test_acc 0.7121\t dur 0.36s\t patience 17\n",
      "Epoch 730\t loss 0.6188\t train_acc 0.8152\t val_acc 0.7368\t test_acc 0.7186\t dur 0.36s\t patience 27\n",
      "Epoch 740\t loss 0.6118\t train_acc 0.8147\t val_acc 0.7368\t test_acc 0.7181\t dur 0.36s\t patience 37\n",
      "Epoch   756: reducing learning rate of group 0 to 1.6807e-03.\n",
      "Epoch 750\t loss 0.6129\t train_acc 0.8144\t val_acc 0.7344\t test_acc 0.7123\t dur 0.36s\t patience 47\n",
      "Epoch 760\t loss 0.6145\t train_acc 0.8151\t val_acc 0.7360\t test_acc 0.7137\t dur 0.36s\t patience 57\n",
      "Epoch 770\t loss 0.6197\t train_acc 0.8146\t val_acc 0.7355\t test_acc 0.7175\t dur 0.36s\t patience 67\n",
      "Epoch 780\t loss 0.6147\t train_acc 0.8143\t val_acc 0.7349\t test_acc 0.7121\t dur 0.36s\t patience 77\n",
      "Epoch 790\t loss 0.6196\t train_acc 0.8151\t val_acc 0.7370\t test_acc 0.7178\t dur 0.36s\t patience 87\n",
      "Epoch 800\t loss 0.6211\t train_acc 0.8145\t val_acc 0.7355\t test_acc 0.7136\t dur 0.36s\t patience 4\n",
      "Epoch 810\t loss 0.6091\t train_acc 0.8153\t val_acc 0.7369\t test_acc 0.7177\t dur 0.37s\t patience 14\n",
      "Epoch 820\t loss 0.6132\t train_acc 0.8142\t val_acc 0.7328\t test_acc 0.7103\t dur 0.36s\t patience 24\n",
      "Epoch 830\t loss 0.6119\t train_acc 0.8153\t val_acc 0.7380\t test_acc 0.7200\t dur 0.37s\t patience 7\n",
      "Epoch 840\t loss 0.6134\t train_acc 0.8148\t val_acc 0.7346\t test_acc 0.7121\t dur 0.36s\t patience 17\n",
      "Epoch 850\t loss 0.6146\t train_acc 0.8150\t val_acc 0.7378\t test_acc 0.7211\t dur 0.36s\t patience 27\n",
      "Epoch 860\t loss 0.6161\t train_acc 0.8149\t val_acc 0.7352\t test_acc 0.7128\t dur 0.36s\t patience 37\n",
      "Epoch 870\t loss 0.6164\t train_acc 0.8150\t val_acc 0.7352\t test_acc 0.7142\t dur 0.36s\t patience 8\n",
      "Epoch 880\t loss 0.6132\t train_acc 0.8145\t val_acc 0.7357\t test_acc 0.7151\t dur 0.36s\t patience 18\n",
      "Epoch 890\t loss 0.6094\t train_acc 0.8144\t val_acc 0.7344\t test_acc 0.7123\t dur 0.36s\t patience 28\n",
      "Epoch   907: reducing learning rate of group 0 to 1.1765e-03.\n",
      "Epoch 900\t loss 0.6149\t train_acc 0.8151\t val_acc 0.7368\t test_acc 0.7186\t dur 0.36s\t patience 38\n",
      "Epoch 910\t loss 0.6060\t train_acc 0.8146\t val_acc 0.7354\t test_acc 0.7156\t dur 0.36s\t patience 48\n",
      "Epoch 920\t loss 0.6112\t train_acc 0.8155\t val_acc 0.7370\t test_acc 0.7178\t dur 0.36s\t patience 58\n",
      "Epoch 930\t loss 0.6116\t train_acc 0.8154\t val_acc 0.7362\t test_acc 0.7152\t dur 0.36s\t patience 68\n",
      "Epoch 940\t loss 0.6158\t train_acc 0.8151\t val_acc 0.7370\t test_acc 0.7179\t dur 0.36s\t patience 78\n",
      "Epoch 950\t loss 0.6094\t train_acc 0.8151\t val_acc 0.7361\t test_acc 0.7172\t dur 0.36s\t patience 88\n",
      "Epoch 960\t loss 0.6044\t train_acc 0.8156\t val_acc 0.7372\t test_acc 0.7194\t dur 0.36s\t patience 98\n",
      "Epoch 970\t loss 0.6095\t train_acc 0.8150\t val_acc 0.7361\t test_acc 0.7175\t dur 0.36s\t patience 108\n",
      "Epoch 980\t loss 0.6149\t train_acc 0.8153\t val_acc 0.7365\t test_acc 0.7172\t dur 0.37s\t patience 118\n",
      "Epoch 990\t loss 0.6158\t train_acc 0.8151\t val_acc 0.7361\t test_acc 0.7162\t dur 0.37s\t patience 128\n",
      "Epoch 1000\t loss 0.6075\t train_acc 0.8151\t val_acc 0.7379\t test_acc 0.7192\t dur 0.37s\t patience 138\n"
     ]
    }
   ],
   "source": [
    "dur = []\n",
    "best_score = 0.\n",
    "best_epoch = 0\n",
    "num_patient_epochs = 0\n",
    "model_folder = './saved_models/'\n",
    "model_path = model_folder + \"model.pt\"\n",
    "log_path = \"log.txt\"\n",
    "\n",
    "def printw(line):\n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(line+\"\\n\")\n",
    "    print(line)\n",
    "\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    \n",
    "print(\"Warming up for {:d} epochs...\".format(warmup_epochs))\n",
    "t0 = time.time()\n",
    "for _ in range(warmup_epochs):\n",
    "    loss, pred = train(model, graph, labels, train_idx, optimizer)\n",
    "    # optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "dur = time.time() - t0 \n",
    "print(\"Done in {:.2f} sec\".format(dur))\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    loss, pred = train(model, graph, labels, train_idx, optimizer)\n",
    "    acc = compute_acc(pred[train_idx], labels[train_idx], evaluator)\n",
    "    train_acc, val_acc, test_acc, train_loss, val_loss, test_loss = evaluate(model, graph, labels, train_idx, val_idx, test_idx, evaluator)\n",
    "    # optimizer.step()\n",
    "    # scheduler.step(test_acc)\n",
    "    dur = time.time() - t0   \n",
    "\n",
    "    # Early stop\n",
    "    if test_acc > best_score:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        best_score = test_acc\n",
    "        best_epoch = epoch\n",
    "        num_patient_epochs = 0\n",
    "        scheduler.step(test_acc)\n",
    "    else:\n",
    "        num_patient_epochs += 1\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "    if (epoch%log_every) == 0:\n",
    "        printw('Epoch {:d}\\t loss {:.4f}\\t train_acc {:.4f}\\t val_acc {:.4f}\\t test_acc {:.4f}\\t dur {:.2f}s\\t patience {:d}'.format(epoch, loss, train_acc, val_acc,test_acc, dur, num_patient_epochs))\n",
    "\n",
    "    if num_patient_epochs == patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:49:20.014231Z",
     "iopub.status.busy": "2020-09-16T21:49:20.012417Z",
     "iopub.status.idle": "2020-09-16T21:49:20.017047Z",
     "shell.execute_reply": "2020-09-16T21:49:20.016404Z"
    },
    "papermill": {
     "duration": 0.097973,
     "end_time": "2020-09-16T21:49:20.017152",
     "exception": false,
     "start_time": "2020-09-16T21:49:19.919179",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test score: 0.7215603975063267\n",
      "at862\n"
     ]
    }
   ],
   "source": [
    "printw(\"Best test score: \" +  str(best_score))\n",
    "printw(\"at\"+ str(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.087548,
     "end_time": "2020-09-16T21:49:20.191658",
     "exception": false,
     "start_time": "2020-09-16T21:49:20.104110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 400.837846,
   "end_time": "2020-09-16T21:49:21.857705",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-16T21:42:41.019859",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
