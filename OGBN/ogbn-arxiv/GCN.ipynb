{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:50:20.594762Z",
     "iopub.status.busy": "2020-09-16T21:50:20.593635Z",
     "iopub.status.idle": "2020-09-16T21:50:39.763650Z",
     "shell.execute_reply": "2020-09-16T21:50:39.762515Z"
    },
    "papermill": {
     "duration": 19.183176,
     "end_time": "2020-09-16T21:50:39.763790",
     "exception": false,
     "start_time": "2020-09-16T21:50:20.580614",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dgl-cu101\r\n",
      "  Downloading dgl_cu101-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.5 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 25.5 MB 9.2 MB/s \r\n",
      "\u001b[?25hCollecting ogb\r\n",
      "  Downloading ogb-1.2.3-py3-none-any.whl (55 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 55 kB 2.5 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (1.18.5)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (2.23.0)\r\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (2.4)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.5.1)\r\n",
      "Collecting outdated>=0.2.0\r\n",
      "  Downloading outdated-0.2.0.tar.gz (4.0 kB)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.1.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.23.2)\r\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.24.3)\r\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.45.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.14.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl-cu101) (2.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl-cu101) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl-cu101) (2020.6.20)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.1->dgl-cu101) (4.4.2)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->ogb) (0.18.2)\r\n",
      "Collecting littleutils\r\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\r\n",
      "Building wheels for collected packages: outdated, littleutils\r\n",
      "  Building wheel for outdated (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for outdated: filename=outdated-0.2.0-py3-none-any.whl size=4960 sha256=d33888118d9898648de466e35f77dcea33af93d390906e97ff8360c432b2a43c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6f/cd/a2/e49170b2cf59e88b952f3414f25a54d9f16f033bded4aaab26\r\n",
      "  Building wheel for littleutils (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=b6d56cc9e0181a11626685658f7aac2dcd2e1a1b325a32bc93550dd09550f770\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\r\n",
      "Successfully built outdated littleutils\r\n",
      "Installing collected packages: dgl-cu101, littleutils, outdated, ogb\r\n",
      "Successfully installed dgl-cu101-0.5.2 littleutils-0.2.2 ogb-1.2.3 outdated-0.2.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "! pip install dgl-cu101 ogb\n",
    "ROOT = \"/kaggle/input/dgl-ogbnarxiv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:50:39.849483Z",
     "iopub.status.busy": "2020-09-16T21:50:39.848430Z",
     "iopub.status.idle": "2020-09-16T21:50:43.507701Z",
     "shell.execute_reply": "2020-09-16T21:50:43.506770Z"
    },
    "papermill": {
     "duration": 3.707329,
     "end_time": "2020-09-16T21:50:43.507848",
     "exception": false,
     "start_time": "2020-09-16T21:50:39.800519",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from dgl.data.utils import load_graphs\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "from dgl.nn import GraphConv, SAGEConv\n",
    "from ogb.nodeproppred import Evaluator\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "x,_ = load_graphs(ROOT + \"data.bin\")\n",
    "graph = x[0]\n",
    "\n",
    "train_idx = torch.load(ROOT + 'train_idx.pt')\n",
    "test_idx = torch.load(ROOT + 'test_idx.pt')\n",
    "val_idx = torch.load(ROOT + 'val_idx.pt')\n",
    "labels = torch.load(ROOT + 'labels.pt')\n",
    "\n",
    "splitted_idx = {'train':train_idx, 'test':test_idx, 'valid':val_idx}\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:50:43.588358Z",
     "iopub.status.busy": "2020-09-16T21:50:43.587374Z",
     "iopub.status.idle": "2020-09-16T21:50:43.609125Z",
     "shell.execute_reply": "2020-09-16T21:50:43.609664Z"
    },
    "papermill": {
     "duration": 0.066228,
     "end_time": "2020-09-16T21:50:43.609824",
     "exception": false,
     "start_time": "2020-09-16T21:50:43.543596",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 40\n"
     ]
    }
   ],
   "source": [
    "in_feats = graph.ndata[\"feat\"].shape[1]\n",
    "n_classes = (labels.max() + 1).item()\n",
    "print (in_feats, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:50:43.710028Z",
     "iopub.status.busy": "2020-09-16T21:50:43.706982Z",
     "iopub.status.idle": "2020-09-16T21:50:43.713057Z",
     "shell.execute_reply": "2020-09-16T21:50:43.713580Z"
    },
    "papermill": {
     "duration": 0.067272,
     "end_time": "2020-09-16T21:50:43.713741",
     "exception": false,
     "start_time": "2020-09-16T21:50:43.646469",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    print ('random seed set to be ' + str(seed))\n",
    "    \n",
    "def cross_entropy(x, labels):\n",
    "    x = x.to(device)\n",
    "    labels = labels.to(device)\n",
    "    y = F.cross_entropy(x, labels[:, 0], reduction=\"none\")\n",
    "    y = th.log(0.5 + y) - math.log(0.5)\n",
    "    return th.mean(y)\n",
    "\n",
    "def compute_acc(pred, labels, evaluator):\n",
    "    return evaluator.eval({\"y_pred\": pred.argmax(dim=-1, keepdim=True), \"y_true\": labels})[\"acc\"]\n",
    "\n",
    "def add_labels(feat, labels, idx):\n",
    "    onehot = th.zeros([feat.shape[0], n_classes]).to(device)\n",
    "    onehot[idx, labels[idx, 0]] = 1\n",
    "    return th.cat([feat, onehot], dim=-1)\n",
    "\n",
    "def train(model, graph, labels, train_idx, optimizer):\n",
    "    model.train()\n",
    "    feat = graph.ndata[\"feat\"]\n",
    "    mask_rate = 0.5\n",
    "    mask = th.rand(train_idx.shape) < mask_rate\n",
    "    train_labels_idx = train_idx[mask]\n",
    "    train_pred_idx = train_idx[~mask]\n",
    "    feat = add_labels(feat, labels, train_labels_idx)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(graph, feat)\n",
    "    loss = cross_entropy(pred[train_pred_idx], labels[train_pred_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, pred\n",
    "\n",
    "@th.no_grad()\n",
    "def evaluate(model, graph, labels, train_idx, val_idx, test_idx, evaluator):\n",
    "    model.eval()\n",
    "    feat = graph.ndata[\"feat\"]\n",
    "    feat = add_labels(feat, labels, train_idx)\n",
    "    pred = model(graph, feat)\n",
    "    train_loss = cross_entropy(pred[train_idx], labels[train_idx])\n",
    "    val_loss = cross_entropy(pred[val_idx], labels[val_idx])\n",
    "    test_loss = cross_entropy(pred[test_idx], labels[test_idx])\n",
    "\n",
    "    return (\n",
    "        compute_acc(pred[train_idx], labels[train_idx], evaluator),\n",
    "        compute_acc(pred[val_idx], labels[val_idx], evaluator),\n",
    "        compute_acc(pred[test_idx], labels[test_idx], evaluator),\n",
    "        train_loss,\n",
    "        val_loss,\n",
    "        test_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:50:43.803611Z",
     "iopub.status.busy": "2020-09-16T21:50:43.802611Z",
     "iopub.status.idle": "2020-09-16T21:50:44.178377Z",
     "shell.execute_reply": "2020-09-16T21:50:44.179095Z"
    },
    "papermill": {
     "duration": 0.429218,
     "end_time": "2020-09-16T21:50:44.179252",
     "exception": false,
     "start_time": "2020-09-16T21:50:43.750034",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed set to be 0\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(0)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,in_feats,out_feats,num_channels=8):\n",
    "        super(Block, self).__init__()\n",
    "        self.gc = GraphConv(in_feats, out_feats)\n",
    "        \n",
    "    def forward(self, g, node_state):\n",
    "        node_states = self.gc(g, node_state)\n",
    "        node_states = F.relu(node_states)\n",
    "        node_states = F.dropout(node_states, p=0.5, training=self.training)\n",
    "        return node_states\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, out_feats,num_paths=2):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_paths = num_paths\n",
    "        self.init = Block(in_feats, n_hidden)\n",
    "        self.lyrs_0 = nn.ModuleList()\n",
    "        self.lyrs_1 = nn.ModuleList()\n",
    "        for _ in range(5):\n",
    "            self.lyrs_0.append(Block(n_hidden, n_hidden))\n",
    "        #self.lyrs_1.append(Block(n_hidden, n_hidden//2))\n",
    "        #self.lyrs_1.append(Block(n_hidden//2, n_hidden))\n",
    "        self.lin = nn.Linear(n_hidden, out_feats)\n",
    "        \n",
    "    def forward(self, g, feat):\n",
    "        node_state = feat\n",
    "        out = _out = self.init(g, node_state)\n",
    "        for i in range(5):\n",
    "            out = self.lyrs_0[i](g, out)\n",
    "        # for lyr in self.lyrs_1:\n",
    "        #     _out = lyr(g, _out)\n",
    "            \n",
    "        # out = torch.stack([out, _out],dim=1).sum(1)\n",
    "        \n",
    "        out = self.lin(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:50:44.265741Z",
     "iopub.status.busy": "2020-09-16T21:50:44.264881Z",
     "iopub.status.idle": "2020-09-16T21:50:50.080615Z",
     "shell.execute_reply": "2020-09-16T21:50:50.079893Z"
    },
    "papermill": {
     "duration": 5.862647,
     "end_time": "2020-09-16T21:50:50.080758",
     "exception": false,
     "start_time": "2020-09-16T21:50:44.218111",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduler min_lr 0.001\n"
     ]
    }
   ],
   "source": [
    "warmup_epochs = 10\n",
    "num_epochs = 1000\n",
    "patience = 250\n",
    "log_every = 10\n",
    "lr = 1e-2\n",
    "weight_decay = 0\n",
    "\n",
    "model = Net(in_feats=128 + n_classes, n_hidden=256, out_feats=40).to(device)\n",
    "graph = graph.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "min_lr = 1e-3\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.7, patience=150,  verbose=True, cooldown=0, min_lr=min_lr)\n",
    "print ('scheduler min_lr', min_lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "evaluator = Evaluator('ogbn-arxiv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:50:50.181251Z",
     "iopub.status.busy": "2020-09-16T21:50:50.180048Z",
     "iopub.status.idle": "2020-09-16T21:56:37.208537Z",
     "shell.execute_reply": "2020-09-16T21:56:37.207642Z"
    },
    "papermill": {
     "duration": 347.08735,
     "end_time": "2020-09-16T21:56:37.208685",
     "exception": false,
     "start_time": "2020-09-16T21:50:50.121335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 epochs...\n",
      "Done in 3.90 sec\n",
      "Epoch 10\t loss 1.8995\t train_acc 0.1791\t val_acc 0.0763\t test_acc 0.0586\t dur 0.41s\t patience 9\n",
      "Epoch 20\t loss 1.6894\t train_acc 0.2784\t val_acc 0.3000\t test_acc 0.2682\t dur 0.41s\t patience 2\n",
      "Epoch 30\t loss 1.6209\t train_acc 0.3225\t val_acc 0.3256\t test_acc 0.2902\t dur 0.41s\t patience 0\n",
      "Epoch 40\t loss 1.4995\t train_acc 0.3847\t val_acc 0.3654\t test_acc 0.3211\t dur 0.42s\t patience 5\n",
      "Epoch 50\t loss 1.4038\t train_acc 0.4147\t val_acc 0.4507\t test_acc 0.4287\t dur 0.41s\t patience 0\n",
      "Epoch 60\t loss 1.3451\t train_acc 0.4468\t val_acc 0.4699\t test_acc 0.4503\t dur 0.42s\t patience 0\n",
      "Epoch 70\t loss 1.2855\t train_acc 0.4735\t val_acc 0.4991\t test_acc 0.4868\t dur 0.41s\t patience 1\n",
      "Epoch 80\t loss 1.2296\t train_acc 0.5063\t val_acc 0.5278\t test_acc 0.5219\t dur 0.41s\t patience 2\n",
      "Epoch 90\t loss 1.1691\t train_acc 0.5563\t val_acc 0.5714\t test_acc 0.5700\t dur 0.41s\t patience 0\n",
      "Epoch 100\t loss 1.1075\t train_acc 0.5783\t val_acc 0.5740\t test_acc 0.5639\t dur 0.41s\t patience 1\n",
      "Epoch 110\t loss 1.0501\t train_acc 0.6089\t val_acc 0.5897\t test_acc 0.5678\t dur 0.41s\t patience 2\n",
      "Epoch 120\t loss 0.9890\t train_acc 0.6441\t val_acc 0.6218\t test_acc 0.6060\t dur 0.41s\t patience 3\n",
      "Epoch 130\t loss 0.9409\t train_acc 0.6726\t val_acc 0.6488\t test_acc 0.6375\t dur 0.41s\t patience 0\n",
      "Epoch 140\t loss 0.9049\t train_acc 0.6915\t val_acc 0.6570\t test_acc 0.6407\t dur 0.59s\t patience 2\n",
      "Epoch   152: reducing learning rate of group 0 to 7.0000e-03.\n",
      "Epoch 150\t loss 0.8560\t train_acc 0.7094\t val_acc 0.6682\t test_acc 0.6506\t dur 0.41s\t patience 3\n",
      "Epoch 160\t loss 0.8385\t train_acc 0.7205\t val_acc 0.6818\t test_acc 0.6712\t dur 0.41s\t patience 0\n",
      "Epoch 170\t loss 0.8182\t train_acc 0.7282\t val_acc 0.6863\t test_acc 0.6753\t dur 0.41s\t patience 0\n",
      "Epoch 180\t loss 0.8071\t train_acc 0.7360\t val_acc 0.6866\t test_acc 0.6738\t dur 0.41s\t patience 3\n",
      "Epoch 190\t loss 0.7830\t train_acc 0.7438\t val_acc 0.6893\t test_acc 0.6722\t dur 0.41s\t patience 13\n",
      "Epoch 200\t loss 0.7706\t train_acc 0.7510\t val_acc 0.6961\t test_acc 0.6811\t dur 0.41s\t patience 3\n",
      "Epoch 210\t loss 0.7630\t train_acc 0.7562\t val_acc 0.6997\t test_acc 0.6856\t dur 0.41s\t patience 1\n",
      "Epoch 220\t loss 0.7603\t train_acc 0.7587\t val_acc 0.6986\t test_acc 0.6775\t dur 0.41s\t patience 4\n",
      "Epoch 230\t loss 0.7496\t train_acc 0.7616\t val_acc 0.7046\t test_acc 0.6888\t dur 0.41s\t patience 4\n",
      "Epoch 240\t loss 0.7389\t train_acc 0.7638\t val_acc 0.7070\t test_acc 0.6924\t dur 0.41s\t patience 14\n",
      "Epoch 250\t loss 0.7372\t train_acc 0.7684\t val_acc 0.7117\t test_acc 0.6964\t dur 0.41s\t patience 24\n",
      "Epoch 260\t loss 0.7313\t train_acc 0.7693\t val_acc 0.7148\t test_acc 0.7008\t dur 0.41s\t patience 0\n",
      "Epoch 270\t loss 0.7213\t train_acc 0.7711\t val_acc 0.7115\t test_acc 0.6916\t dur 0.43s\t patience 2\n",
      "Epoch 280\t loss 0.7195\t train_acc 0.7735\t val_acc 0.7166\t test_acc 0.7018\t dur 0.41s\t patience 1\n",
      "Epoch 290\t loss 0.7213\t train_acc 0.7741\t val_acc 0.7161\t test_acc 0.7007\t dur 0.41s\t patience 11\n",
      "Epoch   303: reducing learning rate of group 0 to 4.9000e-03.\n",
      "Epoch 300\t loss 0.7180\t train_acc 0.7742\t val_acc 0.7141\t test_acc 0.6964\t dur 0.41s\t patience 6\n",
      "Epoch 310\t loss 0.7117\t train_acc 0.7757\t val_acc 0.7135\t test_acc 0.6974\t dur 0.41s\t patience 8\n",
      "Epoch 320\t loss 0.7036\t train_acc 0.7758\t val_acc 0.7193\t test_acc 0.7069\t dur 0.41s\t patience 3\n",
      "Epoch 330\t loss 0.6991\t train_acc 0.7770\t val_acc 0.7160\t test_acc 0.6990\t dur 0.42s\t patience 13\n",
      "Epoch 340\t loss 0.7066\t train_acc 0.7780\t val_acc 0.7218\t test_acc 0.7097\t dur 0.41s\t patience 0\n",
      "Epoch 350\t loss 0.7013\t train_acc 0.7774\t val_acc 0.7147\t test_acc 0.6956\t dur 0.41s\t patience 10\n",
      "Epoch 360\t loss 0.7002\t train_acc 0.7783\t val_acc 0.7176\t test_acc 0.7011\t dur 0.41s\t patience 20\n",
      "Epoch 370\t loss 0.7028\t train_acc 0.7795\t val_acc 0.7197\t test_acc 0.7069\t dur 0.41s\t patience 30\n",
      "Epoch 380\t loss 0.6969\t train_acc 0.7782\t val_acc 0.7148\t test_acc 0.6954\t dur 0.41s\t patience 2\n",
      "Epoch 390\t loss 0.6991\t train_acc 0.7804\t val_acc 0.7209\t test_acc 0.7061\t dur 0.41s\t patience 12\n",
      "Epoch 400\t loss 0.6988\t train_acc 0.7804\t val_acc 0.7153\t test_acc 0.6939\t dur 0.41s\t patience 1\n",
      "Epoch 410\t loss 0.6878\t train_acc 0.7818\t val_acc 0.7206\t test_acc 0.7051\t dur 0.41s\t patience 11\n",
      "Epoch 420\t loss 0.6928\t train_acc 0.7831\t val_acc 0.7224\t test_acc 0.7073\t dur 0.41s\t patience 21\n",
      "Epoch 430\t loss 0.6932\t train_acc 0.7830\t val_acc 0.7210\t test_acc 0.6994\t dur 0.42s\t patience 31\n",
      "Epoch 440\t loss 0.6868\t train_acc 0.7831\t val_acc 0.7210\t test_acc 0.7045\t dur 0.42s\t patience 41\n",
      "Epoch   454: reducing learning rate of group 0 to 3.4300e-03.\n",
      "Epoch 450\t loss 0.6846\t train_acc 0.7833\t val_acc 0.7208\t test_acc 0.7032\t dur 0.42s\t patience 51\n",
      "Epoch 460\t loss 0.6790\t train_acc 0.7832\t val_acc 0.7227\t test_acc 0.7064\t dur 0.41s\t patience 61\n",
      "Epoch 470\t loss 0.6796\t train_acc 0.7845\t val_acc 0.7225\t test_acc 0.7051\t dur 0.41s\t patience 71\n",
      "Epoch 480\t loss 0.6827\t train_acc 0.7843\t val_acc 0.7248\t test_acc 0.7100\t dur 0.41s\t patience 81\n",
      "Epoch 490\t loss 0.6776\t train_acc 0.7845\t val_acc 0.7231\t test_acc 0.7084\t dur 0.41s\t patience 5\n",
      "Epoch 500\t loss 0.6776\t train_acc 0.7842\t val_acc 0.7235\t test_acc 0.7088\t dur 0.43s\t patience 15\n",
      "Epoch 510\t loss 0.6847\t train_acc 0.7846\t val_acc 0.7233\t test_acc 0.7099\t dur 0.42s\t patience 25\n",
      "Epoch 520\t loss 0.6752\t train_acc 0.7846\t val_acc 0.7214\t test_acc 0.7053\t dur 0.41s\t patience 35\n",
      "Epoch 530\t loss 0.6781\t train_acc 0.7852\t val_acc 0.7211\t test_acc 0.7007\t dur 0.41s\t patience 2\n",
      "Epoch 540\t loss 0.6817\t train_acc 0.7833\t val_acc 0.7193\t test_acc 0.6991\t dur 0.41s\t patience 12\n",
      "Epoch 550\t loss 0.6777\t train_acc 0.7835\t val_acc 0.7227\t test_acc 0.7078\t dur 0.41s\t patience 22\n",
      "Epoch 560\t loss 0.6720\t train_acc 0.7849\t val_acc 0.7240\t test_acc 0.7062\t dur 0.41s\t patience 32\n",
      "Epoch 570\t loss 0.6704\t train_acc 0.7836\t val_acc 0.7199\t test_acc 0.7001\t dur 0.41s\t patience 42\n",
      "Epoch 580\t loss 0.6755\t train_acc 0.7843\t val_acc 0.7215\t test_acc 0.7035\t dur 0.45s\t patience 3\n",
      "Epoch 590\t loss 0.6731\t train_acc 0.7858\t val_acc 0.7248\t test_acc 0.7115\t dur 0.43s\t patience 13\n",
      "Epoch   605: reducing learning rate of group 0 to 2.4010e-03.\n",
      "Epoch 600\t loss 0.6627\t train_acc 0.7857\t val_acc 0.7241\t test_acc 0.7094\t dur 0.41s\t patience 23\n",
      "Epoch 610\t loss 0.6721\t train_acc 0.7860\t val_acc 0.7252\t test_acc 0.7092\t dur 0.41s\t patience 33\n",
      "Epoch 620\t loss 0.6657\t train_acc 0.7861\t val_acc 0.7261\t test_acc 0.7110\t dur 0.41s\t patience 43\n",
      "Epoch 630\t loss 0.6643\t train_acc 0.7855\t val_acc 0.7225\t test_acc 0.7065\t dur 0.41s\t patience 53\n",
      "Epoch 640\t loss 0.6678\t train_acc 0.7864\t val_acc 0.7249\t test_acc 0.7102\t dur 0.41s\t patience 63\n",
      "Epoch 650\t loss 0.6728\t train_acc 0.7862\t val_acc 0.7225\t test_acc 0.7033\t dur 0.41s\t patience 73\n",
      "Epoch 660\t loss 0.6668\t train_acc 0.7870\t val_acc 0.7244\t test_acc 0.7095\t dur 0.41s\t patience 83\n",
      "Epoch 670\t loss 0.6681\t train_acc 0.7866\t val_acc 0.7247\t test_acc 0.7093\t dur 0.43s\t patience 93\n",
      "Epoch 680\t loss 0.6690\t train_acc 0.7859\t val_acc 0.7230\t test_acc 0.7077\t dur 0.41s\t patience 103\n",
      "Epoch 690\t loss 0.6599\t train_acc 0.7866\t val_acc 0.7251\t test_acc 0.7085\t dur 0.41s\t patience 113\n",
      "Epoch 700\t loss 0.6728\t train_acc 0.7868\t val_acc 0.7224\t test_acc 0.7045\t dur 0.41s\t patience 123\n",
      "Epoch 710\t loss 0.6682\t train_acc 0.7866\t val_acc 0.7220\t test_acc 0.7029\t dur 0.41s\t patience 133\n",
      "Epoch 720\t loss 0.6676\t train_acc 0.7868\t val_acc 0.7228\t test_acc 0.7038\t dur 0.42s\t patience 143\n",
      "Epoch 730\t loss 0.6643\t train_acc 0.7866\t val_acc 0.7219\t test_acc 0.7036\t dur 0.42s\t patience 153\n",
      "Epoch 740\t loss 0.6664\t train_acc 0.7870\t val_acc 0.7214\t test_acc 0.7046\t dur 0.42s\t patience 163\n",
      "Epoch   756: reducing learning rate of group 0 to 1.6807e-03.\n",
      "Epoch 750\t loss 0.6626\t train_acc 0.7876\t val_acc 0.7235\t test_acc 0.7068\t dur 0.44s\t patience 173\n",
      "Epoch 760\t loss 0.6639\t train_acc 0.7870\t val_acc 0.7228\t test_acc 0.7079\t dur 0.41s\t patience 183\n",
      "Epoch 770\t loss 0.6575\t train_acc 0.7866\t val_acc 0.7236\t test_acc 0.7089\t dur 0.41s\t patience 193\n",
      "Epoch 780\t loss 0.6647\t train_acc 0.7877\t val_acc 0.7239\t test_acc 0.7080\t dur 0.41s\t patience 203\n",
      "Epoch 790\t loss 0.6664\t train_acc 0.7866\t val_acc 0.7247\t test_acc 0.7093\t dur 0.41s\t patience 213\n",
      "Epoch 800\t loss 0.6584\t train_acc 0.7869\t val_acc 0.7243\t test_acc 0.7105\t dur 0.40s\t patience 223\n",
      "Epoch 810\t loss 0.6588\t train_acc 0.7869\t val_acc 0.7237\t test_acc 0.7088\t dur 0.40s\t patience 233\n",
      "Epoch 820\t loss 0.6625\t train_acc 0.7875\t val_acc 0.7237\t test_acc 0.7054\t dur 0.40s\t patience 243\n"
     ]
    }
   ],
   "source": [
    "dur = []\n",
    "best_score = 0.\n",
    "best_epoch = 0\n",
    "num_patient_epochs = 0\n",
    "model_folder = './saved_models/'\n",
    "model_path = model_folder + \"model.pt\"\n",
    "log_path = \"log.txt\"\n",
    "\n",
    "def printw(line):\n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(line+\"\\n\")\n",
    "    print(line)\n",
    "\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    \n",
    "print(\"Warming up for {:d} epochs...\".format(warmup_epochs))\n",
    "t0 = time.time()\n",
    "for _ in range(warmup_epochs):\n",
    "    loss, pred = train(model, graph, labels, train_idx, optimizer)\n",
    "    # optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "dur = time.time() - t0 \n",
    "print(\"Done in {:.2f} sec\".format(dur))\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    loss, pred = train(model, graph, labels, train_idx, optimizer)\n",
    "    acc = compute_acc(pred[train_idx], labels[train_idx], evaluator)\n",
    "    train_acc, val_acc, test_acc, train_loss, val_loss, test_loss = evaluate(model, graph, labels, train_idx, val_idx, test_idx, evaluator)\n",
    "    # optimizer.step()\n",
    "    # scheduler.step(test_acc)\n",
    "    dur = time.time() - t0   \n",
    "\n",
    "    # Early stop\n",
    "    if test_acc > best_score:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        best_score = test_acc\n",
    "        best_epoch = epoch\n",
    "        num_patient_epochs = 0\n",
    "        scheduler.step(test_acc)\n",
    "    else:\n",
    "        num_patient_epochs += 1\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "    if (epoch%log_every) == 0:\n",
    "        printw('Epoch {:d}\\t loss {:.4f}\\t train_acc {:.4f}\\t val_acc {:.4f}\\t test_acc {:.4f}\\t dur {:.2f}s\\t patience {:d}'.format(epoch, loss, train_acc, val_acc,test_acc, dur, num_patient_epochs))\n",
    "\n",
    "    if num_patient_epochs == patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-16T21:56:37.364583Z",
     "iopub.status.busy": "2020-09-16T21:56:37.363697Z",
     "iopub.status.idle": "2020-09-16T21:56:37.369117Z",
     "shell.execute_reply": "2020-09-16T21:56:37.369806Z"
    },
    "papermill": {
     "duration": 0.086153,
     "end_time": "2020-09-16T21:56:37.369998",
     "exception": false,
     "start_time": "2020-09-16T21:56:37.283845",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test score: 0.7150793160915992\n",
      "at577\n"
     ]
    }
   ],
   "source": [
    "printw(\"Best test score: \" +  str(best_score))\n",
    "printw(\"at\"+ str(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.073177,
     "end_time": "2020-09-16T21:56:37.517626",
     "exception": false,
     "start_time": "2020-09-16T21:56:37.444449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 383.176929,
   "end_time": "2020-09-16T21:56:38.733769",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-16T21:50:15.556840",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
