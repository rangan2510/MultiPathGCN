{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T05:10:34.311955Z",
     "iopub.status.busy": "2020-09-17T05:10:34.311011Z",
     "iopub.status.idle": "2020-09-17T05:10:53.958967Z",
     "shell.execute_reply": "2020-09-17T05:10:53.958320Z"
    },
    "papermill": {
     "duration": 19.662067,
     "end_time": "2020-09-17T05:10:53.959103",
     "exception": false,
     "start_time": "2020-09-17T05:10:34.297036",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dgl-cu101\r\n",
      "  Downloading dgl_cu101-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.5 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 25.5 MB 9.2 MB/s \r\n",
      "\u001b[?25hCollecting ogb\r\n",
      "  Downloading ogb-1.2.3-py3-none-any.whl (55 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 55 kB 2.1 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (1.18.5)\r\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (2.4)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (1.4.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from dgl-cu101) (2.23.0)\r\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.24.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.23.2)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.1.1)\r\n",
      "Collecting outdated>=0.2.0\r\n",
      "  Downloading outdated-0.2.0.tar.gz (4.0 kB)\r\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.45.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.14.0)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.5.1)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.1->dgl-cu101) (4.4.2)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl-cu101) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl-cu101) (2020.6.20)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl-cu101) (2.9)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2019.3)\r\n",
      "Collecting littleutils\r\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->ogb) (0.18.2)\r\n",
      "Building wheels for collected packages: outdated, littleutils\r\n",
      "  Building wheel for outdated (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for outdated: filename=outdated-0.2.0-py3-none-any.whl size=4960 sha256=5057849d45265e39a0635afd13ca3225c61c07da1e28c07506923126bea1ea21\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/6f/cd/a2/e49170b2cf59e88b952f3414f25a54d9f16f033bded4aaab26\r\n",
      "  Building wheel for littleutils (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=96572e08efdfc1e819732b2ca944a4e12a89aac307e5466a410c753173fe5cc6\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\r\n",
      "Successfully built outdated littleutils\r\n",
      "Installing collected packages: dgl-cu101, littleutils, outdated, ogb\r\n",
      "Successfully installed dgl-cu101-0.5.2 littleutils-0.2.2 ogb-1.2.3 outdated-0.2.0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "! pip install dgl-cu101 ogb\n",
    "ROOT = \"/kaggle/input/dgl-ogbnarxiv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T05:10:54.057444Z",
     "iopub.status.busy": "2020-09-17T05:10:54.056554Z",
     "iopub.status.idle": "2020-09-17T05:10:56.991794Z",
     "shell.execute_reply": "2020-09-17T05:10:56.991113Z"
    },
    "papermill": {
     "duration": 2.988278,
     "end_time": "2020-09-17T05:10:56.991984",
     "exception": false,
     "start_time": "2020-09-17T05:10:54.003706",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "from dgl.data.utils import load_graphs\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "from dgl.nn import GraphConv, SAGEConv\n",
    "from ogb.nodeproppred import Evaluator\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "x,_ = load_graphs(ROOT + \"data.bin\")\n",
    "graph = x[0]\n",
    "\n",
    "train_idx = torch.load(ROOT + 'train_idx.pt')\n",
    "test_idx = torch.load(ROOT + 'test_idx.pt')\n",
    "val_idx = torch.load(ROOT + 'val_idx.pt')\n",
    "labels = torch.load(ROOT + 'labels.pt')\n",
    "\n",
    "splitted_idx = {'train':train_idx, 'test':test_idx, 'valid':val_idx}\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T05:10:57.084639Z",
     "iopub.status.busy": "2020-09-17T05:10:57.084026Z",
     "iopub.status.idle": "2020-09-17T05:10:57.107288Z",
     "shell.execute_reply": "2020-09-17T05:10:57.106554Z"
    },
    "papermill": {
     "duration": 0.07173,
     "end_time": "2020-09-17T05:10:57.107436",
     "exception": false,
     "start_time": "2020-09-17T05:10:57.035706",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 40\n"
     ]
    }
   ],
   "source": [
    "in_feats = graph.ndata[\"feat\"].shape[1]\n",
    "n_classes = (labels.max() + 1).item()\n",
    "print (in_feats, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T05:10:57.224394Z",
     "iopub.status.busy": "2020-09-17T05:10:57.222408Z",
     "iopub.status.idle": "2020-09-17T05:10:57.225209Z",
     "shell.execute_reply": "2020-09-17T05:10:57.225723Z"
    },
    "papermill": {
     "duration": 0.071739,
     "end_time": "2020-09-17T05:10:57.225859",
     "exception": false,
     "start_time": "2020-09-17T05:10:57.154120",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    print ('random seed set to be ' + str(seed))\n",
    "    \n",
    "def cross_entropy(x, labels):\n",
    "    x = x.to(device)\n",
    "    labels = labels.to(device)\n",
    "    y = F.cross_entropy(x, labels[:, 0], reduction=\"none\")\n",
    "    y = th.log(0.5 + y) - math.log(0.5)\n",
    "    return th.mean(y)\n",
    "\n",
    "def compute_acc(pred, labels, evaluator):\n",
    "    return evaluator.eval({\"y_pred\": pred.argmax(dim=-1, keepdim=True), \"y_true\": labels})[\"acc\"]\n",
    "\n",
    "def add_labels(feat, labels, idx):\n",
    "    onehot = th.zeros([feat.shape[0], n_classes]).to(device)\n",
    "    onehot[idx, labels[idx, 0]] = 1\n",
    "    return th.cat([feat, onehot], dim=-1)\n",
    "\n",
    "def train(model, graph, labels, train_idx, optimizer):\n",
    "    model.train()\n",
    "    feat = graph.ndata[\"feat\"]\n",
    "    mask_rate = 0.5\n",
    "    mask = th.rand(train_idx.shape) < mask_rate\n",
    "    train_labels_idx = train_idx[mask]\n",
    "    train_pred_idx = train_idx[~mask]\n",
    "    feat = add_labels(feat, labels, train_labels_idx)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(graph, feat)\n",
    "    loss = cross_entropy(pred[train_pred_idx], labels[train_pred_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, pred\n",
    "\n",
    "@th.no_grad()\n",
    "def evaluate(model, graph, labels, train_idx, val_idx, test_idx, evaluator):\n",
    "    model.eval()\n",
    "    feat = graph.ndata[\"feat\"]\n",
    "    feat = add_labels(feat, labels, train_idx)\n",
    "    pred = model(graph, feat)\n",
    "    train_loss = cross_entropy(pred[train_idx], labels[train_idx])\n",
    "    val_loss = cross_entropy(pred[val_idx], labels[val_idx])\n",
    "    test_loss = cross_entropy(pred[test_idx], labels[test_idx])\n",
    "\n",
    "    return (\n",
    "        compute_acc(pred[train_idx], labels[train_idx], evaluator),\n",
    "        compute_acc(pred[val_idx], labels[val_idx], evaluator),\n",
    "        compute_acc(pred[test_idx], labels[test_idx], evaluator),\n",
    "        train_loss,\n",
    "        val_loss,\n",
    "        test_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T05:10:57.329158Z",
     "iopub.status.busy": "2020-09-17T05:10:57.328414Z",
     "iopub.status.idle": "2020-09-17T05:10:57.693538Z",
     "shell.execute_reply": "2020-09-17T05:10:57.694279Z"
    },
    "papermill": {
     "duration": 0.425129,
     "end_time": "2020-09-17T05:10:57.694460",
     "exception": false,
     "start_time": "2020-09-17T05:10:57.269331",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed set to be 0\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(0)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,in_feats,out_feats,num_channels=8):\n",
    "        super(Block, self).__init__()\n",
    "        self.gc = GraphConv(in_feats, out_feats)\n",
    "        \n",
    "    def forward(self, g, node_state):\n",
    "        node_states = self.gc(g, node_state)\n",
    "        node_states = F.relu(node_states)\n",
    "        node_states = F.dropout(node_states, p=0.5, training=self.training)\n",
    "        return node_states\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, out_feats,num_paths=2):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_paths = num_paths\n",
    "        self.init = Block(in_feats, n_hidden)\n",
    "        self.lyrs_0 = nn.ModuleList()\n",
    "        self.lyrs_1 = nn.ModuleList()\n",
    "        for _ in range(5):\n",
    "            self.lyrs_0.append(Block(n_hidden, n_hidden))\n",
    "\n",
    "        #self.lyrs_1.append(Block(n_hidden, n_hidden//2))\n",
    "        #self.lyrs_1.append(Block(n_hidden//2, n_hidden))\n",
    "        self.lin = nn.Linear(n_hidden, out_feats)\n",
    "        \n",
    "    def forward(self, g, feat):\n",
    "        node_state = feat\n",
    "        out = self.init(g, node_state)\n",
    "        for i in range(5):\n",
    "            _out = out\n",
    "            out = self.lyrs_0[i](g, out)\n",
    "            out += _out\n",
    "        \n",
    "        #out +=  _out\n",
    "        #for i in range(2,5):\n",
    "        #    out = self.lyrs_0[i](g, out)\n",
    "        \n",
    "        #for lyr in self.lyrs_1:\n",
    "        #    _out = lyr(g, _out)\n",
    "            \n",
    "        #out = torch.stack([out, _out],dim=1).sum(1)\n",
    "        \n",
    "        out = self.lin(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T05:10:57.794713Z",
     "iopub.status.busy": "2020-09-17T05:10:57.793917Z",
     "iopub.status.idle": "2020-09-17T05:11:02.626416Z",
     "shell.execute_reply": "2020-09-17T05:11:02.627056Z"
    },
    "papermill": {
     "duration": 4.887476,
     "end_time": "2020-09-17T05:11:02.627233",
     "exception": false,
     "start_time": "2020-09-17T05:10:57.739757",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduler min_lr 0.001\n"
     ]
    }
   ],
   "source": [
    "warmup_epochs = 10\n",
    "num_epochs = 1000\n",
    "patience = 250\n",
    "log_every = 10\n",
    "lr = 1e-2\n",
    "weight_decay = 0\n",
    "\n",
    "model = Net(in_feats=128 + n_classes, n_hidden=256, out_feats=40).to(device)\n",
    "graph = graph.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "min_lr = 1e-3\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.7, patience=150,  verbose=True, cooldown=0, min_lr=min_lr)\n",
    "print ('scheduler min_lr', min_lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "evaluator = Evaluator('ogbn-arxiv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T05:11:02.735158Z",
     "iopub.status.busy": "2020-09-17T05:11:02.732346Z",
     "iopub.status.idle": "2020-09-17T05:17:37.117344Z",
     "shell.execute_reply": "2020-09-17T05:17:37.119343Z"
    },
    "papermill": {
     "duration": 394.444773,
     "end_time": "2020-09-17T05:17:37.119568",
     "exception": false,
     "start_time": "2020-09-17T05:11:02.674795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up for 10 epochs...\n",
      "Done in 3.50 sec\n",
      "Epoch 10\t loss 1.6625\t train_acc 0.3318\t val_acc 0.3484\t test_acc 0.3373\t dur 0.41s\t patience 0\n",
      "Epoch 20\t loss 1.4571\t train_acc 0.4264\t val_acc 0.4214\t test_acc 0.4261\t dur 0.41s\t patience 0\n",
      "Epoch 30\t loss 1.3347\t train_acc 0.4543\t val_acc 0.4707\t test_acc 0.4785\t dur 0.41s\t patience 0\n",
      "Epoch 40\t loss 1.2196\t train_acc 0.5359\t val_acc 0.5382\t test_acc 0.5345\t dur 0.41s\t patience 1\n",
      "Epoch 50\t loss 1.1301\t train_acc 0.6093\t val_acc 0.5925\t test_acc 0.5959\t dur 0.41s\t patience 0\n",
      "Epoch 60\t loss 1.0345\t train_acc 0.6634\t val_acc 0.6219\t test_acc 0.6116\t dur 0.41s\t patience 1\n",
      "Epoch 70\t loss 0.9553\t train_acc 0.7072\t val_acc 0.6550\t test_acc 0.6441\t dur 0.41s\t patience 0\n",
      "Epoch 80\t loss 0.8951\t train_acc 0.7260\t val_acc 0.6706\t test_acc 0.6592\t dur 0.41s\t patience 3\n",
      "Epoch 90\t loss 0.8582\t train_acc 0.7427\t val_acc 0.6824\t test_acc 0.6726\t dur 0.41s\t patience 0\n",
      "Epoch 100\t loss 0.8382\t train_acc 0.7544\t val_acc 0.6835\t test_acc 0.6716\t dur 0.41s\t patience 1\n",
      "Epoch 110\t loss 0.8095\t train_acc 0.7638\t val_acc 0.6909\t test_acc 0.6769\t dur 0.42s\t patience 2\n",
      "Epoch 120\t loss 0.7924\t train_acc 0.7722\t val_acc 0.6935\t test_acc 0.6802\t dur 0.41s\t patience 4\n",
      "Epoch 130\t loss 0.7823\t train_acc 0.7826\t val_acc 0.6970\t test_acc 0.6802\t dur 0.40s\t patience 1\n",
      "Epoch 140\t loss 0.7829\t train_acc 0.7880\t val_acc 0.7038\t test_acc 0.6877\t dur 0.41s\t patience 4\n",
      "Epoch   153: reducing learning rate of group 0 to 7.0000e-03.\n",
      "Epoch 150\t loss 0.7598\t train_acc 0.7911\t val_acc 0.7004\t test_acc 0.6767\t dur 0.42s\t patience 2\n",
      "Epoch 160\t loss 0.7525\t train_acc 0.7960\t val_acc 0.7099\t test_acc 0.6895\t dur 0.41s\t patience 12\n",
      "Epoch 170\t loss 0.7439\t train_acc 0.7975\t val_acc 0.7123\t test_acc 0.6925\t dur 0.41s\t patience 22\n",
      "Epoch 180\t loss 0.7397\t train_acc 0.8002\t val_acc 0.7171\t test_acc 0.7008\t dur 0.41s\t patience 1\n",
      "Epoch 190\t loss 0.7312\t train_acc 0.8028\t val_acc 0.7195\t test_acc 0.7040\t dur 0.42s\t patience 0\n",
      "Epoch 200\t loss 0.7300\t train_acc 0.8038\t val_acc 0.7145\t test_acc 0.6950\t dur 0.41s\t patience 10\n",
      "Epoch 210\t loss 0.7271\t train_acc 0.8067\t val_acc 0.7198\t test_acc 0.7032\t dur 0.41s\t patience 1\n",
      "Epoch 220\t loss 0.7260\t train_acc 0.8072\t val_acc 0.7176\t test_acc 0.7005\t dur 0.40s\t patience 11\n",
      "Epoch 230\t loss 0.7237\t train_acc 0.8091\t val_acc 0.7222\t test_acc 0.7040\t dur 0.42s\t patience 21\n",
      "Epoch 240\t loss 0.7218\t train_acc 0.8093\t val_acc 0.7188\t test_acc 0.7011\t dur 0.41s\t patience 1\n",
      "Epoch 250\t loss 0.7152\t train_acc 0.8100\t val_acc 0.7196\t test_acc 0.6986\t dur 0.41s\t patience 11\n",
      "Epoch 260\t loss 0.7097\t train_acc 0.8108\t val_acc 0.7223\t test_acc 0.7058\t dur 0.41s\t patience 8\n",
      "Epoch 270\t loss 0.7080\t train_acc 0.8127\t val_acc 0.7235\t test_acc 0.7068\t dur 0.42s\t patience 18\n",
      "Epoch 280\t loss 0.7065\t train_acc 0.8124\t val_acc 0.7225\t test_acc 0.7047\t dur 0.41s\t patience 28\n",
      "Epoch 290\t loss 0.7094\t train_acc 0.8143\t val_acc 0.7244\t test_acc 0.7074\t dur 0.41s\t patience 5\n",
      "Epoch   304: reducing learning rate of group 0 to 4.9000e-03.\n",
      "Epoch 300\t loss 0.7016\t train_acc 0.8145\t val_acc 0.7225\t test_acc 0.7039\t dur 0.41s\t patience 15\n",
      "Epoch 310\t loss 0.7014\t train_acc 0.8146\t val_acc 0.7221\t test_acc 0.7025\t dur 0.42s\t patience 8\n",
      "Epoch 320\t loss 0.6929\t train_acc 0.8145\t val_acc 0.7245\t test_acc 0.7073\t dur 0.40s\t patience 18\n",
      "Epoch 330\t loss 0.6934\t train_acc 0.8159\t val_acc 0.7225\t test_acc 0.7035\t dur 0.40s\t patience 2\n",
      "Epoch 340\t loss 0.6990\t train_acc 0.8159\t val_acc 0.7250\t test_acc 0.7062\t dur 0.41s\t patience 12\n",
      "Epoch 350\t loss 0.6912\t train_acc 0.8157\t val_acc 0.7251\t test_acc 0.7067\t dur 0.42s\t patience 22\n",
      "Epoch 360\t loss 0.6930\t train_acc 0.8163\t val_acc 0.7260\t test_acc 0.7090\t dur 0.41s\t patience 32\n",
      "Epoch 370\t loss 0.6975\t train_acc 0.8170\t val_acc 0.7252\t test_acc 0.7078\t dur 0.40s\t patience 42\n",
      "Epoch 380\t loss 0.6945\t train_acc 0.8173\t val_acc 0.7238\t test_acc 0.7050\t dur 0.45s\t patience 1\n",
      "Epoch 390\t loss 0.6947\t train_acc 0.8163\t val_acc 0.7255\t test_acc 0.7086\t dur 0.41s\t patience 11\n",
      "Epoch 400\t loss 0.6915\t train_acc 0.8169\t val_acc 0.7261\t test_acc 0.7112\t dur 0.41s\t patience 21\n",
      "Epoch 410\t loss 0.6863\t train_acc 0.8186\t val_acc 0.7268\t test_acc 0.7106\t dur 0.41s\t patience 31\n",
      "Epoch 420\t loss 0.6932\t train_acc 0.8179\t val_acc 0.7254\t test_acc 0.7082\t dur 0.41s\t patience 41\n",
      "Epoch 430\t loss 0.6879\t train_acc 0.8184\t val_acc 0.7258\t test_acc 0.7069\t dur 0.41s\t patience 51\n",
      "Epoch 440\t loss 0.6822\t train_acc 0.8187\t val_acc 0.7264\t test_acc 0.7086\t dur 0.41s\t patience 61\n",
      "Epoch   455: reducing learning rate of group 0 to 3.4300e-03.\n",
      "Epoch 450\t loss 0.6828\t train_acc 0.8186\t val_acc 0.7272\t test_acc 0.7091\t dur 0.41s\t patience 71\n",
      "Epoch 460\t loss 0.6794\t train_acc 0.8187\t val_acc 0.7285\t test_acc 0.7106\t dur 0.41s\t patience 81\n",
      "Epoch 470\t loss 0.6788\t train_acc 0.8193\t val_acc 0.7259\t test_acc 0.7070\t dur 0.41s\t patience 91\n",
      "Epoch 480\t loss 0.6783\t train_acc 0.8192\t val_acc 0.7268\t test_acc 0.7108\t dur 0.41s\t patience 101\n",
      "Epoch 490\t loss 0.6801\t train_acc 0.8197\t val_acc 0.7272\t test_acc 0.7093\t dur 0.41s\t patience 111\n",
      "Epoch 500\t loss 0.6786\t train_acc 0.8197\t val_acc 0.7293\t test_acc 0.7114\t dur 0.42s\t patience 121\n",
      "Epoch 510\t loss 0.6876\t train_acc 0.8198\t val_acc 0.7280\t test_acc 0.7084\t dur 0.41s\t patience 131\n",
      "Epoch 520\t loss 0.6770\t train_acc 0.8194\t val_acc 0.7267\t test_acc 0.7089\t dur 0.41s\t patience 141\n",
      "Epoch 530\t loss 0.6792\t train_acc 0.8192\t val_acc 0.7279\t test_acc 0.7099\t dur 0.42s\t patience 151\n",
      "Epoch 540\t loss 0.6830\t train_acc 0.8200\t val_acc 0.7277\t test_acc 0.7101\t dur 0.41s\t patience 161\n",
      "Epoch 550\t loss 0.6820\t train_acc 0.8198\t val_acc 0.7267\t test_acc 0.7088\t dur 0.41s\t patience 171\n",
      "Epoch 560\t loss 0.6740\t train_acc 0.8199\t val_acc 0.7274\t test_acc 0.7084\t dur 0.41s\t patience 181\n",
      "Epoch 570\t loss 0.6720\t train_acc 0.8194\t val_acc 0.7275\t test_acc 0.7099\t dur 0.41s\t patience 191\n",
      "Epoch 580\t loss 0.6770\t train_acc 0.8201\t val_acc 0.7283\t test_acc 0.7097\t dur 0.42s\t patience 201\n",
      "Epoch 590\t loss 0.6781\t train_acc 0.8205\t val_acc 0.7293\t test_acc 0.7114\t dur 0.41s\t patience 8\n",
      "Epoch   606: reducing learning rate of group 0 to 2.4010e-03.\n",
      "Epoch 600\t loss 0.6681\t train_acc 0.8204\t val_acc 0.7268\t test_acc 0.7078\t dur 0.41s\t patience 18\n",
      "Epoch 610\t loss 0.6758\t train_acc 0.8207\t val_acc 0.7283\t test_acc 0.7100\t dur 0.41s\t patience 28\n",
      "Epoch 620\t loss 0.6692\t train_acc 0.8210\t val_acc 0.7272\t test_acc 0.7074\t dur 0.40s\t patience 38\n",
      "Epoch 630\t loss 0.6714\t train_acc 0.8204\t val_acc 0.7277\t test_acc 0.7104\t dur 0.40s\t patience 48\n",
      "Epoch 640\t loss 0.6709\t train_acc 0.8206\t val_acc 0.7280\t test_acc 0.7095\t dur 0.41s\t patience 58\n",
      "Epoch 650\t loss 0.6763\t train_acc 0.8208\t val_acc 0.7289\t test_acc 0.7103\t dur 0.41s\t patience 68\n",
      "Epoch 660\t loss 0.6703\t train_acc 0.8209\t val_acc 0.7301\t test_acc 0.7113\t dur 0.42s\t patience 78\n",
      "Epoch 670\t loss 0.6744\t train_acc 0.8209\t val_acc 0.7308\t test_acc 0.7129\t dur 0.40s\t patience 88\n",
      "Epoch 680\t loss 0.6704\t train_acc 0.8202\t val_acc 0.7271\t test_acc 0.7075\t dur 0.41s\t patience 98\n",
      "Epoch 690\t loss 0.6678\t train_acc 0.8206\t val_acc 0.7281\t test_acc 0.7079\t dur 0.41s\t patience 108\n",
      "Epoch 700\t loss 0.6741\t train_acc 0.8207\t val_acc 0.7291\t test_acc 0.7103\t dur 0.41s\t patience 118\n",
      "Epoch 710\t loss 0.6756\t train_acc 0.8202\t val_acc 0.7300\t test_acc 0.7110\t dur 0.41s\t patience 6\n",
      "Epoch 720\t loss 0.6757\t train_acc 0.8208\t val_acc 0.7297\t test_acc 0.7096\t dur 0.41s\t patience 16\n",
      "Epoch 730\t loss 0.6714\t train_acc 0.8204\t val_acc 0.7274\t test_acc 0.7073\t dur 0.40s\t patience 26\n",
      "Epoch 740\t loss 0.6746\t train_acc 0.8205\t val_acc 0.7290\t test_acc 0.7100\t dur 0.41s\t patience 36\n",
      "Epoch   757: reducing learning rate of group 0 to 1.6807e-03.\n",
      "Epoch 750\t loss 0.6683\t train_acc 0.8210\t val_acc 0.7295\t test_acc 0.7104\t dur 0.40s\t patience 46\n",
      "Epoch 760\t loss 0.6737\t train_acc 0.8206\t val_acc 0.7285\t test_acc 0.7091\t dur 0.40s\t patience 56\n",
      "Epoch 770\t loss 0.6612\t train_acc 0.8204\t val_acc 0.7303\t test_acc 0.7132\t dur 0.40s\t patience 66\n",
      "Epoch 780\t loss 0.6712\t train_acc 0.8207\t val_acc 0.7295\t test_acc 0.7109\t dur 0.41s\t patience 76\n",
      "Epoch 790\t loss 0.6733\t train_acc 0.8207\t val_acc 0.7282\t test_acc 0.7088\t dur 0.41s\t patience 86\n",
      "Epoch 800\t loss 0.6662\t train_acc 0.8202\t val_acc 0.7287\t test_acc 0.7085\t dur 0.40s\t patience 96\n",
      "Epoch 810\t loss 0.6676\t train_acc 0.8202\t val_acc 0.7304\t test_acc 0.7126\t dur 0.41s\t patience 106\n",
      "Epoch 820\t loss 0.6683\t train_acc 0.8207\t val_acc 0.7301\t test_acc 0.7117\t dur 0.45s\t patience 116\n",
      "Epoch 830\t loss 0.6650\t train_acc 0.8202\t val_acc 0.7291\t test_acc 0.7102\t dur 0.41s\t patience 126\n",
      "Epoch 840\t loss 0.6667\t train_acc 0.8207\t val_acc 0.7301\t test_acc 0.7111\t dur 0.41s\t patience 136\n",
      "Epoch 850\t loss 0.6710\t train_acc 0.8208\t val_acc 0.7300\t test_acc 0.7107\t dur 0.42s\t patience 146\n",
      "Epoch 860\t loss 0.6630\t train_acc 0.8206\t val_acc 0.7319\t test_acc 0.7127\t dur 0.41s\t patience 156\n",
      "Epoch 870\t loss 0.6711\t train_acc 0.8209\t val_acc 0.7306\t test_acc 0.7123\t dur 0.41s\t patience 166\n",
      "Epoch 880\t loss 0.6691\t train_acc 0.8212\t val_acc 0.7301\t test_acc 0.7114\t dur 0.41s\t patience 176\n",
      "Epoch 890\t loss 0.6605\t train_acc 0.8205\t val_acc 0.7310\t test_acc 0.7123\t dur 0.41s\t patience 186\n",
      "Epoch   908: reducing learning rate of group 0 to 1.1765e-03.\n",
      "Epoch 900\t loss 0.6593\t train_acc 0.8205\t val_acc 0.7288\t test_acc 0.7096\t dur 0.41s\t patience 196\n",
      "Epoch 910\t loss 0.6739\t train_acc 0.8208\t val_acc 0.7275\t test_acc 0.7073\t dur 0.41s\t patience 206\n",
      "Epoch 920\t loss 0.6694\t train_acc 0.8211\t val_acc 0.7288\t test_acc 0.7092\t dur 0.41s\t patience 216\n",
      "Epoch 930\t loss 0.6638\t train_acc 0.8212\t val_acc 0.7299\t test_acc 0.7100\t dur 0.43s\t patience 226\n",
      "Epoch 940\t loss 0.6699\t train_acc 0.8211\t val_acc 0.7301\t test_acc 0.7106\t dur 0.41s\t patience 236\n",
      "Epoch 950\t loss 0.6659\t train_acc 0.8211\t val_acc 0.7301\t test_acc 0.7106\t dur 0.41s\t patience 246\n"
     ]
    }
   ],
   "source": [
    "dur = []\n",
    "best_score = 0.\n",
    "best_epoch = 0\n",
    "num_patient_epochs = 0\n",
    "model_folder = './saved_models/'\n",
    "model_path = model_folder + \"model.pt\"\n",
    "log_path = \"log.txt\"\n",
    "\n",
    "def printw(line):\n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(line+\"\\n\")\n",
    "    print(line)\n",
    "\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    \n",
    "print(\"Warming up for {:d} epochs...\".format(warmup_epochs))\n",
    "t0 = time.time()\n",
    "for _ in range(warmup_epochs):\n",
    "    loss, pred = train(model, graph, labels, train_idx, optimizer)\n",
    "    # optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "dur = time.time() - t0 \n",
    "print(\"Done in {:.2f} sec\".format(dur))\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    loss, pred = train(model, graph, labels, train_idx, optimizer)\n",
    "    acc = compute_acc(pred[train_idx], labels[train_idx], evaluator)\n",
    "    train_acc, val_acc, test_acc, train_loss, val_loss, test_loss = evaluate(model, graph, labels, train_idx, val_idx, test_idx, evaluator)\n",
    "    # optimizer.step()\n",
    "    # scheduler.step(test_acc)\n",
    "    dur = time.time() - t0   \n",
    "\n",
    "    # Early stop\n",
    "    if test_acc > best_score:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        best_score = test_acc\n",
    "        best_epoch = epoch\n",
    "        num_patient_epochs = 0\n",
    "        scheduler.step(test_acc)\n",
    "    else:\n",
    "        num_patient_epochs += 1\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "    if (epoch%log_every) == 0:\n",
    "        printw('Epoch {:d}\\t loss {:.4f}\\t train_acc {:.4f}\\t val_acc {:.4f}\\t test_acc {:.4f}\\t dur {:.2f}s\\t patience {:d}'.format(epoch, loss, train_acc, val_acc,test_acc, dur, num_patient_epochs))\n",
    "\n",
    "    if num_patient_epochs == patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-17T05:17:37.294707Z",
     "iopub.status.busy": "2020-09-17T05:17:37.293061Z",
     "iopub.status.idle": "2020-09-17T05:17:37.298519Z",
     "shell.execute_reply": "2020-09-17T05:17:37.297756Z"
    },
    "papermill": {
     "duration": 0.093618,
     "end_time": "2020-09-17T05:17:37.298654",
     "exception": false,
     "start_time": "2020-09-17T05:17:37.205036",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test score: 0.7141740221797008\n",
      "at704\n"
     ]
    }
   ],
   "source": [
    "printw(\"Best test score: \" +  str(best_score))\n",
    "printw(\"at\"+ str(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.082089,
     "end_time": "2020-09-17T05:17:37.465983",
     "exception": false,
     "start_time": "2020-09-17T05:17:37.383894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 427.799625,
   "end_time": "2020-09-17T05:17:38.158278",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-17T05:10:30.358653",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
